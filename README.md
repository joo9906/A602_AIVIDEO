# Training 폴더의 주피터 노트북이 AI 모델 코드입니다.
### 혹시라도 사용해보시려면 test/chmera_test.py를 실행하시면 직접 해보실 수 있습니다. 모델과 인코더를 함께 업로드 하였습니다. .env를 만드시고 test/model의 모델과 인코더 경로를 넣으시면 됩니다.

### 하드웨어
싸피에서 제공해주는 GPU 서버를 활용하였습니다.
GPU 모델은  Tesla V100-PCIE-32GB 입니다.

## 모델 구성
### 모델
    - 모델 파일 : training/model_optimizing.ipynb 
    - 하이퍼 파라미터 저장 파일 : training/config.py
1. TimeDistributed 1D CNN을 선택. 16 + 32 + 64 채널의 3층
2. BiLSTM 256 + 128
3. MultiHeadAttention(head=4, dim=32)로 구성하여 LSTM의 마지막 채널인 128과 맞추었습니다.
4. Flatten

### 관련 내용 정리
0. 성능 
    - test_acc = 0.93, val_acc=0.83 
    - test_loss = 1.5 , val_loss = 1.65 
    - top5는 매우 잘 나옵니다(test에서 0.998, val에서 0.968). 그래서 전처리한 데이터로 테스트하면 매우 잘 나오는데 직접 수어 영상을 넣으면 특정 단어들을 제외하면 top5에도 들어가지 않습니다.
    - 유사한 동작을 하면 특정 단어로 수렴은 잘 하고 있습니다. 이러면 데이터 문제가 맞을지요..?
        - 예를 들어 "감사" 라는 뜻의 수어를 하면 팔을 들어올리는 동작이 있던 없던 짚신 이라는 단어가 나올 확률이 가장 높게, 일관적으로 나오게 됩니다.

1. 시계열을 CNN에 적용하지 않으면 성능이 매우 떨어집니다. 하지만 연산량이 많아지는 단점이 있는데 성능이 너무 급락해서 포기할 수가 없을 것 같습니다. (정확도 20% 이상 차이)

2. GlobalMaxPooling과 Flatten 사이에서 몇 번 테스트를 했는데 Flatten의 성능이 압도적으로 잘 나옵니다(13% 이상 차이)

3. LSTM은 양방향으로 하는게 가장 성능이 좋았지만 val_acc 기준 3% 정도 차이나서 경량화를 생각하면 단방향이 나을까요? 그리고 GRU를 통해 경량화를 할 수 있다고 하는데 테스트를 한 번 간략하게 했을때는 val_acc 0.6 정도의 벽에 가로막혔습니다.

4. 양자화는 성능도 잘 안나오는데 시도하기엔 이르다고 생각하여 시도하지 않았습니다.

## 데이터 전처리

- `data-preprocessing` 폴더에서 `make_dataset.ipynb`는 영상에서 mediapipe로 좌표를 추출하는 코드입니다.
- `data-preprocessing` 폴더에서 `csv_to_npz.ipynb`는 좌표를 모델 input으로 바꾸는 코드입니다.


### 여쭤볼 것
- 모델 보다는 데이터 쪽에서 문제가 있어서 성능이 안나온다고 판단하고 있습니다. 그래서 데이터 부분을 손보려고 합니다.
1. 현재는 정확히 수어 시작 ~ 수어 종료 부분만 학습 데이터로 들어가고 있습니다. 영상 전체에서 손을 올리거나 내리는 의미가 없는 동작을 넣는게 확실히 성능 향상에 도움이 될까요?
2. 추가로 저희가 100가지 정도의 영상을 직접 찍어서 학습 데이터에 추가하려고 하고 있습니다. 어떻게 생각 하시나요
3. 지금으로선 기존의 학습 데이터에 데이터 증강을 적용하고 있는데 이래도 될까요? 아니라면 어디에 증강을 넣어야 할까요? 

### 모델 최적화, 경량화 과정입니다.
https://www.notion.so/2721a5fc266380b9b570c431625cb788?source=copy_link